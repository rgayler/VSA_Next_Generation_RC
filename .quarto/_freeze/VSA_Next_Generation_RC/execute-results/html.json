{
  "hash": "fa48c7cc5b29b712cc5de5d0db3cbf43",
  "result": {
    "markdown": "---\ntitle: \"VSA Next Generation Reservoir Computing\"\ndate: 2023-04-01\nauthor: \"Ross Gayler\"\ntoc: true\nnumber-sections: true\nformat: \n    html: default\n    pdf: default\nbibliography: references.json\n---\n\n\n## Objectives\n\nThe aim of this project is to develop good VSA implementations of Next Generation Reservoir Computing [@gauthierNextGenerationReservoir2021].\nThe sought outcomes are:\n\n-   A good, practical multivariate dynamical system predictor;\n\n-   Further theoretical development of VSA through exposure of current understanding to new practical problems.\n    (I view theoretical development as being about finding productive interpretations/extensions of the abstract basis of VSA.)\n\n## Statistical feature construction\n\n### What does Reservoir Computing do?\n\nReservoir computing uses a recurrently updated reservoir to create a representation vector from a sequence of input values.\nThe reservoir value is transformed to predictions by a simple linear readout transformation, for example see [@gauthierNextGenerationReservoir2021, fig. 1].\n\n### Interpretation as statistical feature construction\n\nThe linear readout is equivalent to the application of a standard statistical regression model.\nA standard univariate regression model is effectively the calculation of the dot product of the vector of regression coefficients and the vector of predictor values.\nA multivariate regression is the concatenation of multiple univariate regressions (so the vector of regression coefficients becomes a matrix of regression coefficients).\nUnder this interpretation, the reservoir is constructing the vector of predictor values to be input to the regression model.\n\n### VSA representation of predictors\n\nIn standard statistics/ML the representation of predictor values is localist.\nHowever, in VSA the natural form of representation is distributed key-value pairs, with the key corresponding to the identity of the predictor and the value corresponding to the value of the predictor [@kanervaFullyDistributedRepresentation1997].\nI *think* that the distributed representations can be viewed as rotations of a localist representation [@qiuGraphEmbeddingsTensor2022, sec. 6.1].\n\nIt's also worth noting that in standard statistics/ML the value of a predictor is represented as the scalar value of the predictor variable.\nThis can be directly implemented by representing the value of the predictor variable by the magnitude of the hypervector (e.g. @kleykoIntegerEchoState2022, sec IV.A.2).\nNote that this represents the predictor as a key rather than a key-value pair.\nAn alternative is to represent the value of a predictor by the direction of a hypervector (e.g. by Fractional Power Encoding).\n\n### Theory of sequence indexing\n\nThe theory of sequence indexing and working memory in recurrent neural networks [@fradyTheorySequenceIndexing2018a] appears to be very relevant to this project, but I haven't read it in detail yet.\nOn the basis of a quick skim this project appears to differ by stressing the algebraic interpretation of the action of the reservoir update.\nIn this project we interpret the reservoir update as algebraic VSA operations.\n\n### Integer Echo State Network\n\nWe'll start by applying this lens (algebraic interpretation of the reservoir update) to the Integer Echo State Network.\nThe reservoir update equation [@kleykoIntegerEchoState2022, eqn. 5] is: $$\\mathbf{x}(n) = f_\\kappa(Sh(\\mathbf{x}(n-1), 1) + \\mathbf{u}^{\\textrm{HD}}(n) + \\mathbf{y}^{\\textrm{HD}}(n-1))$$ where $\\mathbf{x}(n)$ is the value of the hypervector representing the state of the reservoir at time $n$; $f_\\kappa()$ is the clipping function applied to the reservoir hypervector; $Sh(\\mathbf{x}(n-1), 1)$ is the application of a cyclic shift (by one element) permutation to the value of the reservoir hypervector at the previous time step ($n-1$); $\\mathbf{u}^{HD}(n)$ is the hypervector encoding of the input vector at time $n$; $\\mathbf{y}^{HD}(n-1))$ is the is the hypervector encoding of the output vector at the previous time step ($n-1$); and $+$ is hypervector addition (bundling).\n\nSimplify the equation and notation a little for ease of exposition:\n\n-   Drop the clipping function $f_\\kappa()$ (equivalent to setting the clipping threshold $\\kappa$ to a high value).\n-   Replace the cyclic shift function $Sh()$ with a generic, fixed permutation function $\\textrm{P}()$.\n-   Drop the HD superscripts from the input and output hypervectors by interpreting $\\mathbf{u}(n)$ and $\\mathbf{y}(n-1))$ as the hypervector encodings of the input and output vectors at their respective time steps.\n-   Drop the output hypervector $\\mathbf{y}(n-1))$ because the input hypervector $\\mathbf{u}(n)$ can be interpreted as including the output values.\n\nThe simplified equation is: $$\\mathbf{x}(n) = \\textrm{P}(\\mathbf{x}(n-1)) + \\mathbf{u}(n)$$\n\n### Expand repeated application of the reservoir update equation\n\nTrace out the repeated application, starting from time $n = 0$ with an empty reservoir, $\\mathbf{x}(0) = \\mathbf{0}$, where $\\mathbf{0}$ is the additive identity hypervector.\n\n$n = 0 : \\mathbf{x}(0) = \\mathbf{0}$\\\n$n = 1 : \\mathbf{x}(1) = \\textrm{P}(\\mathbf{x}(0)) + \\mathbf{u}(1) = \\textrm{P}(\\mathbf{0}) + \\mathbf{u}(1) = \\mathbf{0} + \\mathbf{u}(1) = \\mathbf{u}(1)$\\\n$n = 2 : \\mathbf{x}(2) = \\textrm{P}(\\mathbf{x}(1)) + \\mathbf{u}(2) = \\textrm{P}(\\mathbf{u}(1)) + \\mathbf{u}(2)$ $n = 3 : \\mathbf{x}(3) = \\textrm{P}(\\mathbf{x}(2)) + \\mathbf{u}(3) = \\textrm{P}(\\textrm{P}(\\mathbf{u}(1)) + \\mathbf{u}(2)) + \\mathbf{u}(3)$\\\n$= \\textrm{P}^2(\\mathbf{u}(1)) + \\textrm{P}(\\mathbf{u}(2)) + \\mathbf{u}(3) = \\textrm{P}^2(\\mathbf{u}(1)) + \\textrm{P}^1(\\mathbf{u}(2)) + \\textrm{P}^0(\\mathbf{u}(3))$\n\nThis is a standard VSA idiom for representation of a sequence of values [@kleykoSurveyHyperdimensionalComputing2022a, eqn. 17].\nThe representation is equivalent to a set of predictors, each of which is a lagged copy of the input values.\nFor example, the $\\textrm{P}^2(\\mathbf{u}())$ term represents the value of the input two time steps earlier.\nAll the permutations are orthogonal, so the permutations effectively constitute the identities of the predictor variables.\nThis means that the integer Echo State Network is restricted to modelling the outputs as weighted sums of the lagged inputs.\n\nNote that the form of the result resembles a polynomial with the powers of the pemutation corresponding to the powers of the variable of the polynomial and the input values corresponding to the coefficients of the polynomial.\nNote also that the result is iteratively built up, one input value at a time by transforming the result so far to reflect it's updated role and bundling in the next input value.\n\n## Polynomial feature construction\n\n### What does Next Generation RC do?\n\nNext Generation Reservoir Computing replaces the recurrent reservoir calculations with the direct calculation of products of delayed input values [@gauthierNextGenerationReservoir2021, fig. 1].\nThis is justified by a universal approximator result showing that the NGRC is equivalent to traditional RC.\nThe NGRC authors see the advantage of this as being the elimination of the reservoir, in particular, the random matrix implementing the reservoir update,\n\nFrom the point of view of a statistical modeller, NGRC is constructing polynomial interaction features from the input stream to use as predictors in a simple regression model.\nWe investigate how to create equivalent VSA polynomial interaction features using VSA mechanisms.\n\n### RC as incremental construction of predictors\n\nWe *could* create a desired set of polynomial predictors in one step, but this would require knowing which specific predictors are required and would require dedicated VSA circuitry for each predictor to be constructed.\nInstead, we will use a reservoir update circuit to construct *all* the polynomial predictors incrementally.\n\nNote that the NGRC objection to the reservoir was actually to the random matrix.\nThe VSA reservoir update does not use a random matrix so we don't have the tuning problems identified by the NGRC authors.\n\nVSA reservoir update relies on superposition and distributivity to construct many predictors in parallel, so it requires much less hardware than dedicated hardware for each predictor.\n\nAlso, note that because of the incremental update the predictor terms are constructed on a specific order.\nThe simplest terms are constructed first and more complex terms later (with smaller magnitude).\nThis constitutes an inductive bias to use lower order terms.\n\n## VSA polynomial reservoir update\n\n### Develop polynomial update formula\n\n#### $(x + 1)^n$\n\nBase the first attempt at the reservoir update on the polynomial $(x + 1)^n$.\nTake the simplified update equation (where $\\times$ is hypervector multiplication (binding)): $$\\mathbf{x}(n) = (\\mathbf{x}(n-1) + \\mathbf{1}) \\times (\\mathbf{u}(n) + \\mathbf{1})$$\n\nTrace out the repeated application, starting from time $n = 0$ with an empty reservoir, $\\mathbf{x}(0) = \\mathbf{0}$.\n\n$\\mathbf{x}(0) = \\mathbf{0}$\\\n$\\mathbf{x}(1) = (\\mathbf{x}(0) + \\mathbf{1}) \\times (\\mathbf{u}(1) + \\mathbf{1}) = (\\mathbf{0} + \\mathbf{1}) \\times (\\mathbf{u}(1) + \\mathbf{1}) = \\mathbf{u}(1) + \\mathbf{1}$\\\n$\\mathbf{x}(2) = (\\mathbf{x}(1) + \\mathbf{1}) \\times (\\mathbf{u}(2) + \\mathbf{1}) = (\\mathbf{u}(1) + \\mathbf{1} + \\mathbf{1}) \\times (\\mathbf{u}(2) + \\mathbf{1})$\\\n$= \\mathbf{u}(1) \\times \\mathbf{u}(2) + 2 \\cdot \\mathbf{u}(2) + \\mathbf{u}(1) + 2\\cdot \\mathbf{1}$\\\n$\\mathbf{x}(3) = (\\mathbf{x}(2) + \\mathbf{1}) \\times (\\mathbf{u}(3) + \\mathbf{1})$\\\n$= (\\mathbf{u}(1) \\times \\mathbf{u}(2) + 2 \\cdot \\mathbf{u}(2) + \\mathbf{u}(1) + 2\\cdot \\mathbf{1} + \\mathbf{1}) \\times (\\mathbf{u}(3) + \\mathbf{1})$\\\n$= (\\mathbf{u}(1) \\times \\mathbf{u}(2) + 2 \\cdot \\mathbf{u}(2) + \\mathbf{u}(1) + 3\\cdot \\mathbf{1}) \\times (\\mathbf{u}(3) + \\mathbf{1})$\\\n$= \\mathbf{u}(1) \\times \\mathbf{u}(2) \\times \\mathbf{u}(3)+ 2 \\cdot \\mathbf{u}(2) \\times \\mathbf{u}(3) + \\mathbf{u}(1) \\times \\mathbf{u}(3) + 3\\cdot \\mathbf{u}(3) + \\mathbf{u}(1) \\times \\mathbf{u}(2) + 2 \\cdot \\mathbf{u}(2) + \\mathbf{u}(1) + 3\\cdot \\mathbf{1}$\\\n\nwhere $\\cdot$ is for clarity when multiplying a scalar by a hypervector.\n\nThis forms product and cross-product terms from lagged values of the input.\nIt retains the old terms across iterations and adds new terms at each iteration.\n\nTo a first approximation the different scalar multipliers of the terms are irrelevant because all the terms will be multiplied by scalar coefficients in the regression readout.\nThese readout coefficients can compensate for arbitrary scalar multipliers of the terms.\n\nThe scalar multipliers determine the relative magnitudes of the hypervectors corresponding to the algebraic terms.\nGiven that the magnitude of the reservoir hypervector will almost certainly be normalised, this means that the terms with the smallest scalar multipliers will have the smallest smallest magnitudes.\nThis raises the possibility that terms with very small magnitudes may become effectively invisible in the noise of the VSA system.\n\n#### Multiplicative identity ($\\mathbf{1}$) and copy arcs\n\nThe multiplicative identity ($\\mathbf{1}$) is in the update formula to copy terms across iterations.\nThe update formula above generates a $\\mathbf{1}$ term as an additive component of the reservoir hypervector at every iteration.\nThis term is uninformative with respect to variation of the predictions.\nGiven that some of the magnitude of the reservoir hypervector is used by the $\\mathbf{1}$ term, that the magnitude of the reservoir hypervector is almost certainly normalised, and that the implementation will be noisy, the presence of the $\\mathbf{1}$ term reduces the information capacity of the reservoir hypervector.\n\nIt would be interesting to see if a polynomial reservoir update can be constructed without using the multiplicative identity term ($\\mathbf{1}$).\nThe key observation here is that binding with the multiplicative identity term is effectively a copy operation in the data VSA flow graph.\nTranslate the reservoir update equation $\\mathbf{x}(n) = (\\mathbf{x}(n-1) + \\mathbf{1}) \\times (\\mathbf{u}(n) + \\mathbf{1})$ to a VSA data flow graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DiagrammeR)\n\nDiagrammeR::grViz(\"\n  digraph graph1 {\n  A -> B\n  }\n  \")\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-8314121d6776045fb1e3\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-8314121d6776045fb1e3\">{\"x\":{\"diagram\":\"\\n  digraph graph1 {\\n  A -> B\\n  }\\n  \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n#### Binding of FPE = vector addition\n\n#### Binding of permuted FPE = path sequence\n\n## Feature fading\n\n-   Implemented bundling is nonassociative\n\n-   Introduce weighted bundling\n\n-   Fading easier to see with randsel bundling\n\n## Development suggestions\n\n### Input encoding\n\n-   It might be advantageous to construct sequence representations in the input streams rather than the reservoir update. This might be equivalent to hierarchical reservoirs - have input stream specific resrvoirs that create input sequence representations and a stream fusion reservoir (or maybe stream fusion doesn't need the recurrent update).\n\n### Intercept term\n\n-   Check on how the intercept term is treated in the reservoir readout.\n\n-   Need better treatment of bundling and capacity.\n    It's not just the number of superposed items (which can be changed arbitrarily by re-bracketing).\n    Also, D-dimensional HRR binding is equivalent to the sum of D-many MAP bindings, but only counts as one item.\n\n### Feature fading\n\n-   Weighted bundling in state update could be used to extend the life of items in the reservoir, but at the cost of reducing the gain of new items added to the bundle.\n\n-   It might be possible for the update formula to change dynamically to indefinitely rpreserve the contents of the reservoir as a working memory, e.g. gating as in LSTM so that the reservoir only cycles when a new item is added.\n\n### Warm up\n\n-   Look at the value of the long-term steady state of the reservoir and the dynamics of approaching steady state, because that speaks to the length of the warm-up phase in RC\n\n-   Is it possible to read-out from the reservoir during the warm-up phase?\n    (use resonator networks?) An interesting life is 100% warm-up.\n\n### Dynamic readout\n\nRather than a fixed-weight readout mechanism, is it possible to have a readout that varies in response to task demands at readout?\nThis relates to generalisation at recall and catastrophic forgetting\n",
    "supporting": [
      "VSA_Next_Generation_RC_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"VSA_Next_Generation_RC_files/libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\n<script src=\"VSA_Next_Generation_RC_files/libs/viz-1.8.2/viz.js\"></script>\n<link href=\"VSA_Next_Generation_RC_files/libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\n<script src=\"VSA_Next_Generation_RC_files/libs/grViz-binding-1.0.9/grViz.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}